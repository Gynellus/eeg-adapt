{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faee31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import braindecode\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717e7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"dataset-h5/KU_mi_smt.h5\"\n",
    "outpath = \"model-predictions\"\n",
    "modelpath = \"pretrained_models\"\n",
    "scheme = 5\n",
    "rate = 100\n",
    "lr = 0.0005\n",
    "dfile = h5py.File(datapath, 'r')\n",
    "torch.cuda.set_device(0)\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c0c6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/s1\" (2 members)>\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(datapath, 'r')\n",
    "print(f[\"s1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e145ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs = [1]\n",
    "\n",
    "\n",
    "# Get data from single subject.\n",
    "def get_data(subj):\n",
    "    dpath = '/s' + str(subj)\n",
    "    X = dfile[\"s1/X\"]\n",
    "    Y = dfile[\"s1/Y\"]\n",
    "    return X[:], Y[:]\n",
    "\n",
    "\n",
    "X, Y = get_data(subjs[0])\n",
    "n_classes = 2\n",
    "in_chans = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c398dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_conv_length = auto ensures we only get a single output in the time dimension\n",
    "model = Deep4Net(in_chans=in_chans, n_classes=n_classes,\n",
    "                 input_time_length=X.shape[2],\n",
    "                 final_conv_length=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eaaa703",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Deep4Net' object has no attribute 'monitors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4e3fb4348224>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m# model.epochs_df.to_csv(pjoin(outpath, 'epochs' + suffix + '.csv'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\HackathonOct22\\lib\\site-packages\\braindecode\\models\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# reset runtime monitor if exists...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"last_call_time\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_call_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Deep4Net' object has no attribute 'monitors'"
     ]
    }
   ],
   "source": [
    "# def reset_conv_pool_block(network, block_nr):\n",
    "#     suffix = \"_{:d}\".format(block_nr)\n",
    "#     conv = getattr(network, 'conv' + suffix)\n",
    "#     kernel_size = conv.kernel_size\n",
    "#     n_filters_before = conv.in_channels\n",
    "#     n_filters = conv.out_channels\n",
    "#     setattr(network, 'conv' + suffix,\n",
    "#             nn.Conv2d(\n",
    "#                 n_filters_before,\n",
    "#                 n_filters,\n",
    "#                 kernel_size,\n",
    "#                 stride=(1, 1),\n",
    "#                 bias=False,\n",
    "#             ))\n",
    "#     setattr(network, 'bnorm' + suffix,\n",
    "#             nn.BatchNorm2d(\n",
    "#                 n_filters,\n",
    "#                 momentum=0.1,\n",
    "#                 affine=True,\n",
    "#                 eps=1e-5,\n",
    "#             ))\n",
    "#     # Initialize the layers.\n",
    "#     conv = getattr(network, 'conv' + suffix)\n",
    "#     bnorm = getattr(network, 'bnorm' + suffix)\n",
    "#     nn.init.xavier_uniform_(conv.weight, gain=1)\n",
    "#     nn.init.constant_(bnorm.weight, 1)\n",
    "#     nn.init.constant_(bnorm.bias, 0)\n",
    "\n",
    "\n",
    "# def reset_model(checkpoint):\n",
    "#     # Load the state dict of the model.\n",
    "#     model.network.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#     # # Resets the last conv block\n",
    "#     # reset_conv_pool_block(model.network, block_nr=4)\n",
    "#     # reset_conv_pool_block(model.network, block_nr=3)\n",
    "#     # reset_conv_pool_block(model.network, block_nr=2)\n",
    "#     # # Resets the fully-connected layer.\n",
    "#     # # Parameters of newly constructed modules have requires_grad=True by default.\n",
    "#     # n_final_conv_length = model.network.conv_classifier.kernel_size[0]\n",
    "#     # n_prev_filter = model.network.conv_classifier.in_channels\n",
    "#     # n_classes = model.network.conv_classifier.out_channels\n",
    "#     # model.network.conv_classifier = nn.Conv2d(\n",
    "#     #     n_prev_filter, n_classes, (n_final_conv_length, 1), bias=True)\n",
    "#     # nn.init.xavier_uniform_(model.network.conv_classifier.weight, gain=1)\n",
    "#     # nn.init.constant_(model.network.conv_classifier.bias, 0)\n",
    "\n",
    "#     if scheme != 5:\n",
    "#         # Freeze all layers.\n",
    "#         for param in model.network.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         if scheme in {1, 2, 3, 4}:\n",
    "#             # Unfreeze the FC layer.\n",
    "#             for param in model.network.conv_classifier.parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#         if scheme in {2, 3, 4}:\n",
    "#             # Unfreeze the conv4 layer.\n",
    "#             for param in model.network.conv_4.parameters():\n",
    "#                 param.requires_grad = True\n",
    "#             for param in model.network.bnorm_4.parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#         if scheme in {3, 4}:\n",
    "#             # Unfreeze the conv3 layer.\n",
    "#             for param in model.network.conv_3.parameters():\n",
    "#                 param.requires_grad = True\n",
    "#             for param in model.network.bnorm_3.parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#         if scheme == 4:\n",
    "#             # Unfreeze the conv2 layer.\n",
    "#             for param in model.network.conv_2.parameters():\n",
    "#                 param.requires_grad = True\n",
    "#             for param in model.network.bnorm_2.parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#     # Only optimize parameters that requires gradient.\n",
    "#     optimizer = AdamW(filter(lambda p: p.requires_grad, model.network.parameters()),\n",
    "#                       lr=lr, weight_decay=0.5*0.001)\n",
    "#     model.compile(loss=F.nll_loss, optimizer=optimizer, extra_monitors = LossMonitor,\n",
    "#                   iterator_seed=20200205, )\n",
    "\n",
    "# cutoff = int(rate * 200 / 100)\n",
    "# # Use only session 1 data for training\n",
    "# assert(cutoff <= 100)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                      lr=lr, weight_decay=0.5*0.001)\n",
    "model.compile(loss=F.nll_loss, optimizer=optimizer, extra_monitors = braindecode.experiments.monitors.LossMonitor,\n",
    "                  iterator_seed=20200205, )\n",
    "\n",
    "\n",
    "fold = 0\n",
    "subj = 1\n",
    "suffix = '_s' + str(subj) + '_f' + str(fold)\n",
    "checkpoint = torch.load(pjoin(modelpath, 'model_f' + str(fold) + '.pt'),\n",
    "                        map_location='cuda:' + '0')\n",
    "# reset_model(checkpoint)\n",
    "\n",
    "# X, Y = get_data(subj)\n",
    "# X_train, Y_train = X[:cutoff], Y[:cutoff]\n",
    "# X_val, Y_val = X[200:300], Y[200:300]\n",
    "# X_test, Y_test = X[300:], Y[300:]\n",
    "# model.fit(X_train, Y_train, epochs=TRAIN_EPOCH,\n",
    "#           batch_size=BATCH_SIZE, scheduler='cosine',\n",
    "#           validation_data=(X_val, Y_val), remember_best_column='valid_loss')\n",
    "# model.epochs_df.to_csv(pjoin(outpath, 'epochs' + suffix + '.csv'))\n",
    "    \n",
    "test_loss = model.evaluate(X, Y)\n",
    "print(test_loss)\n",
    "with open(pjoin(outpath, 'test' + suffix + '.json'), 'w') as f:\n",
    "    json.dump(test_loss, f)\n",
    "\n",
    "dfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7838d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e483b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
